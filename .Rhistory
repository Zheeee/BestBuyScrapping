##################################
library(VIM) #For the visualization and imputation of missing values.
install.packages("quantreg")
##################################
#####Visualizing Missing Data#####
##################################
library(VIM) #For the visualization and imputation of missing values.
install.packages("VIM")
##################################
#####Visualizing Missing Data#####
##################################
library(VIM) #For the visualization and imputation of missing values.
install.packages("quantreg")
##################################
#####Visualizing Missing Data#####
##################################
library(VIM) #For the visualization and imputation of missing values.
nnhelp(sleep) #Inspecting the mammal sleep dataset.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
#Mean value imputation method 2.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
?transform
#Mean value imputation method 3.
library(caret)
install.packages("caret")
#Mean value imputation method 3.
library(caret)
install.packages("ddalpha")
#Mean value imputation method 3.
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
pre
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
install.packages("Hmisc")
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
type(imputed.x2)
class(imputed.x2)
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
missing.data
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
install.packages("RANN")
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
imputed.1nn
#Imputing using 5NN.
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
imputed.5nn
#Imputing using 9NN.
pre.9nn = preProcess(missing.data, method = 'knnImpute', k=9)
imputed.9nn = predict(pre.9nn, missing.data)
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
sqrt(nrow(sleep))
#Using 8 nearest neighbors.
pre.8nn = preProcess(sleep, method = 'knnImpute', k=8)
sleep.imputed8NN = predict(pre.8nn, sleep)
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
install.packages("deldir")
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
iris.imputed12NN
install.packages("kknn")
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
iris.imputed12NN
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
complete
missing
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
missing
complete
missing.vector
missing
install.packages("flexdashboard")
install.packages("RANN")
devtools::install_github('rstudio/rmarkdown')
brands = strsplit(brands, split = ' +')[[1]]
# company name
brands =  '"Asus"      "Apple"     "HP"        "Dell"      "Lenovo"    "Samsung"   "Microsoft" "Acer"      "Alienware"  "MSI"  "Google" "Razer"'
brands = strsplit(brands, split = ' +')[[1]]
brands
brands = gsub('\\"', '')
# company name
brands =  '"Asus"      "Apple"     "HP"        "Dell"      "Lenovo"    "Samsung"   "Microsoft" "Acer"      "Alienware"  "MSI"  "Google" "Razer"'
brands = gsub('\\"', '')
# company name
brands =  '"Asus"      "Apple"     "HP"        "Dell"      "Lenovo"    "Samsung"   "Microsoft" "Acer"      "Alienware"  "MSI"  "Google" "Razer"'
brands = gsub('\\"', '', brands)
brands = strsplit(brands, split = ' +')[[1]]
brands
class(brands)
# company name
brands_name =  '"Asus"      "Apple"     "HP"        "Dell"      "Lenovo"    "Samsung"   "Microsoft" "Acer"      "Alienware"  "MSI"  "Google" "Razer"'
brands_name = gsub('\\"', '', brands_name)
brands_name = strsplit(brands_name, split = ' +')[[1]]
brands = feature_dup %>%
filter(., features %in% brands_name)
setwd("~/Documents/R/BCR/WebScraping")
library(dplyr)
library(shiny)
library(ggplot2)
library(tidyr)
library(stringr)
library(wordcloud)
library(SnowballC)
library(tm)
# import data
reviews = read.csv('./data/reviews.csv', stringsAsFactors = F)
price = read.csv("./data/price.csv", stringsAsFactors = F)
price = price %>%
mutate(., rank = as.numeric(rownames(.)))
# join reviews and price
reviews_price = reviews %>%
left_join(., price, by = "proc_name") %>%
arrange(., rank) %>%
na.omit(.) %>%
mutate(., index = as.integer(rownames(.)))
# create duplicate rows based on laptops' features
temp = reviews_price %>%
select(., proc_name, index) %>%
mutate(., features = sapply(proc_name, function(x) unlist(strsplit(x, split = " - | – | / |- ")))) %>%
mutate(., freq = sapply(features, function(x) length(x)))
temp_expand = temp[rep(rownames(temp), temp$freq), 1:3]
temp_ = temp_expand %>%
mutate(., pos = rownames(.)) %>%
mutate(., pos_ = mapply(function(x, y) ifelse(any(grep('\\.', x) > 0),
as.integer(strsplit(x, split = '\\.')[[1]][1]) - y + 1 + as.integer(strsplit(x, split = '\\.')[[1]][2]),
as.integer(x) - y + 1),
pos, index)) %>%
mutate(., num_row = as.integer(rownames(.))) %>%
mutate(., features = mapply(function(y, z) features[y][[1]][z], num_row, pos_)) %>%
select(., index, features)
# adjust display
temp_$features = gsub('”', '\\"', temp_$features)
disp_size = str_extract(temp_$features, '[1-9]+\\.?[1-9]?\\"')
disp_size = str_extract(disp_size, '[1-9]+\\.?[1-9]?')
# adjust cpu
temp_$features = gsub('Intel Core m3', 'Intel Core M3', temp_$features)
temp_$features = gsub('Intel® Core™ i5', 'Intel Core i5', temp_$features)
temp_ = temp_ %>%
mutate(., features = mapply(function(x,y) ifelse(is.na(y) == T, x, y), features, disp_size))
# merge reviews_price and temp
feature_dup = reviews_price %>%
inner_join(., temp_, by = 'index') %>%
mutate(., my_score = score * (help_ / (help_ + unhelp_))) %>%
filter(., my_score != 0)
# cpu analysis
core = feature_dup %>%
filter(., !grepl('Drive', features)) %>%
filter(., grepl('Intel', features))
# company name
brands_name =  '"Asus"      "Apple"     "HP"        "Dell"      "Lenovo"    "Samsung"   "Microsoft" "Acer"      "Alienware"  "MSI"  "Google" "Razer"'
brands_name = gsub('\\"', '', brands_name)
brands_name = strsplit(brands_name, split = ' +')[[1]]
brands = feature_dup %>%
filter(., features %in% brands_name)
View(brands)
ggplot(data = brands, aes(x = reorder(features, my_score, FUN = median), y = my_score)) +
geom_boxplot()
ggplot(data = brands, aes(x = reorder(features, price, FUN = median), y = price)) +
geom_boxplot()
ggplot(data = brands, aes(x = features)) +
geom_bar()
a brands[brands$features == 'Microsoft', brands$proc_name]
a = brands[brands$features == 'Microsoft', brands$proc_name]
a = brands[brands$features == 'Microsoft',]$proc_name
unique(a)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
# ggplot(data = brands, aes(x = features)) +
#   geom_bar()
# ggplot(data = brands, aes(x = reorder(features, my_score, FUN = median), y = my_score)) +
#   geom_boxplot()
# ggplot(data = brands, aes(x = reorder(features, price, FUN = median), y = price)) +
#   geom_boxplot()
# memory
meo = feature_dup %>%
filter(., grepl('Memory', features))
unique(meo$features)
# adjust memory
temp_$features = gsub('8 GB Memory', '8GB Memory', temp_$features)
# merge reviews_price and temp
feature_dup = reviews_price %>%
inner_join(., temp_, by = 'index') %>%
mutate(., my_score = score * (help_ / (help_ + unhelp_))) %>%
filter(., my_score != 0)
# ggplot(data = brands, aes(x = features)) +
#   geom_bar()
# ggplot(data = brands, aes(x = reorder(features, my_score, FUN = median), y = my_score)) +
#   geom_boxplot()
# ggplot(data = brands, aes(x = reorder(features, price, FUN = median), y = price)) +
#   geom_boxplot()
# memory
meo = feature_dup %>%
filter(., grepl('Memory', features))
unique(meo$features)
# ggplot(data = brands, aes(x = features)) +
#   geom_bar()
# ggplot(data = brands, aes(x = reorder(features, my_score, FUN = median), y = my_score)) +
#   geom_boxplot()
# ggplot(data = brands, aes(x = reorder(features, price, FUN = median), y = price)) +
#   geom_boxplot()
# memory
meo = feature_dup %>%
filter(., grepl('Memory|memory', features))
unique(meo$features)
# ggplot(data = brands, aes(x = features)) +
#   geom_bar()
# ggplot(data = brands, aes(x = reorder(features, my_score, FUN = median), y = my_score)) +
#   geom_boxplot()
# ggplot(data = brands, aes(x = reorder(features, price, FUN = median), y = price)) +
#   geom_boxplot()
# memory
drive = feature_dup %>%
filter(., grepl('Drive|drive', features))
unique(drive$features)
# ggplot(data = brands, aes(x = features)) +
#   geom_bar()
# ggplot(data = brands, aes(x = reorder(features, my_score, FUN = median), y = my_score)) +
#   geom_boxplot()
# ggplot(data = brands, aes(x = reorder(features, price, FUN = median), y = price)) +
#   geom_boxplot()
# memory
drive = feature_dup %>%
filter(., grepl('Drive', features))
unique(drive$features)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
unique(feature_dup$features == 'Google')
unique(feature_dup$features[feature_dup$features == 'Google'])
unique(feature_dup$proc_name[feature_dup$features == 'Google'])
unique(feature_dup$proc_name[feature_dup$features == 'Asus'])
unique(feature_dup$proc_name[feature_dup$features == 'Acer'])
runApp()
setwd("~/Documents/R/BCR/WebScraping")
library(dplyr)
library(shiny)
library(ggplot2)
library(tidyr)
library(stringr)
library(wordcloud)
library(SnowballC)
library(tm)
# import data
reviews = read.csv('./data/reviews.csv', stringsAsFactors = F)
price = read.csv("./data/price.csv", stringsAsFactors = F)
price = price %>%
mutate(., rank = as.numeric(rownames(.)))
# join reviews and price
reviews_price = reviews %>%
left_join(., price, by = "proc_name") %>%
arrange(., rank) %>%
na.omit(.) %>%
mutate(., index = as.integer(rownames(.)))
# create duplicate rows based on laptops' features
temp = reviews_price %>%
select(., proc_name, index) %>%
mutate(., features = sapply(proc_name, function(x) unlist(strsplit(x, split = " - | – | / |- ")))) %>%
mutate(., freq = sapply(features, function(x) length(x)))
temp_expand = temp[rep(rownames(temp), temp$freq), 1:3]
temp_ = temp_expand %>%
mutate(., pos = rownames(.)) %>%
mutate(., pos_ = mapply(function(x, y) ifelse(any(grep('\\.', x) > 0),
as.integer(strsplit(x, split = '\\.')[[1]][1]) - y + 1 + as.integer(strsplit(x, split = '\\.')[[1]][2]),
as.integer(x) - y + 1),
pos, index)) %>%
mutate(., num_row = as.integer(rownames(.))) %>%
mutate(., features = mapply(function(y, z) features[y][[1]][z], num_row, pos_)) %>%
select(., index, features)
# adjust display
temp_$features = gsub('”', '\\"', temp_$features)
disp_size = str_extract(temp_$features, '[1-9]+\\.?[1-9]?\\"')
disp_size = str_extract(disp_size, '[1-9]+\\.?[1-9]?')
# adjust cpu
temp_$features = gsub('Intel Core m3', 'Intel Core M3', temp_$features)
temp_$features = gsub('Intel® Core™ i5', 'Intel Core i5', temp_$features)
# adjust memory
temp_$features = gsub('8 GB Memory', '8GB Memory', temp_$features)
temp_ = temp_ %>%
mutate(., features = mapply(function(x,y) ifelse(is.na(y) == T, x, y), features, disp_size))
# merge reviews_price and temp
feature_dup = reviews_price %>%
inner_join(., temp_, by = 'index')
# filter(., help_ >= unhelp_) %>%
mutate(., my_score = score * (help_ / (help_ + unhelp_))) %>%
filter(., my_score != 0)
# company name
brands_name =  '"Asus"      "Apple"     "HP"        "Dell"      "Lenovo"    "Samsung"   "Microsoft" "Acer"      "Alienware"  "MSI"  "Google" "Razer"'
brands_name = gsub('\\"', '', brands_name)
brands_name = strsplit(brands_name, split = ' +')[[1]]
nrow(feature_dup)
# merge reviews_price and temp
feature_dup = reviews_price %>%
inner_join(., temp_, by = 'index') %>%
# filter(., help_ >= unhelp_) %>%
mutate(., my_score = score * (help_ / (help_ + unhelp_))) %>%
filter(., my_score != 0)
nrow(feature_dup)
# merge reviews_price and temp
feature_dup = reviews_price %>%
inner_join(., temp_, by = 'index') %>%
filter(., help_ >= unhelp_) %>%
mutate(., my_score = score * (help_ / (help_ + unhelp_))) %>%
filter(., my_score != 0)
nrow(feature_dup)
runApp()
View(feature_dup)
class(feature_dup$percent)
# adjust recommand percent
feature_dup$percent = gsub('%', '', feature_dup$percent)
feature_dup$percent = as.numeric(feature_dup$percent)
class(feature_dup$percent)
View(feature_dup)
View(feature_dup)
runApp()
company = feature_dup %>%
filter(., features %in% brands_name)
View(company)
unique(company$features)
runApp()
unique(company$features)
class(company)
unique(company$features)[1]
brands_name
str(company)
?selectizeInput
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
